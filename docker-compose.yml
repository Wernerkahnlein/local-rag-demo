services:
  summary_analyzer:
    build: 
      context: .
      dockerfile: sm.Dockerfile
    volumes:
      - /mnt/c/Users/Mati/Desktop/dev/summaries:/summaries:ro
    networks:
      - shared
    ports:
      - 8082:8082
    environment:
      - SUMMARY_PATH=/summaries
      - ENV=docker
  loader:
    build: 
      context: .
      dockerfile: loader.Dockerfile
    volumes:
      - /mnt/c/Users/Mati/Desktop/dev/summaries:/summaries:ro
    networks:
      - shared
    environment:
      - SUMMARY_PATH=/summaries
      - ENV=docker
    depends_on:
      - qdrant
  qdrant:
    image: qdrant/qdrant
    volumes:
      - /mnt/c/Users/Mati/Desktop/dev/summary_analyzer/qdrant/data:/qdrant/storage:z
    networks:
      - shared
    ports:
      - 6333:6333
  embedder:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    volumes:
      - ./llm/nomic-embed-text-latest.gguf:/models/nomic-embed-text-latest.gguf
    entrypoint: /app/llama-server
    command: -m /models/nomic-embed-text-latest.gguf -n 512 -ngl 30 --port 8080 --embedding --pooling cls -ub 8192
    ports:
      - 8080:8080
    networks:
      - shared
    environment:
      - LLAMA_UBATCH_SIZE=1024
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  chatter:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    volumes:
    - ./llm/deepseek-r1-14b-Q4_K_M.gguf:/models/deepseek-r1-14b-Q4_K_M.gguf
    entrypoint: /app/llama-server
    command: -m /models/deepseek-r1-14b-Q4_K_M.gguf --ctx-size 4096 -n 2048 -ngl 60 --port 8081
    ports:
      - 8081:8081
    networks:
      - shared
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  shared:
    driver: bridge
